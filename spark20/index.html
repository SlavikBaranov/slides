<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

  <title>Spark 2.0: Ожидание | Реальность</title>

  <link rel="stylesheet" href="css/reveal.css">
  <link rel="stylesheet" href="css/theme/white-helv.css">

  <!-- Theme used for syntax highlighting of code -->
  <link rel="stylesheet" href="css/highlight/idea.css">

  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement('link');
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
    document.getElementsByTagName('head')[0].appendChild(link);
  </script>
  <script src="js/plotly-latest.min.js"></script>
  <script src="js/charts.js"></script>
</head>
<body>
<div class="reveal">
  <div class="slides">
    <section>
      <div style="width: 80%; margin: 0 auto; text-align: left;">
        <h1 style="position: relative;">Spark 2.0<br>
          <small>Ожидание | Реальность</small>
          <img src="img/spark-logo.png"
               style="max-width: 21%; border: none; background: none; box-shadow: none;
                                 position: absolute; top: 15px; right: 0px; margin: 0;"/>
        </h1>
        <br>
        <br>
        <br>
        <h2 style="position: relative;">Вячеслав Баранов,<br>
        <small>Одноклассники</small>
        <img src="img/ok-logo.png"
             style="height: 100%; border: none; background: none; box-shadow: none;
                               position: absolute; bottom: 0px; right: 0px; margin: 0;"/>
        </h2>
      </div>
    </section>
    <section>
      <h2>Что нового в 2.0?</h2>
      <ul>
        <li>Datasets</li>
        <li>Structured streaming</li>
        <li>Improvements:</li>
        <ul>
          <li>ANSI SQL Support</li>
          <li>SparkSession</li>
          <li>New Accumulator API</li>
          <li>Stat functions</li>
        </ul>
      </ul>
      <hr>
      <blockquote>
        <h3>
          Хороший релиз<br>
          <small>И фичи интересные</small>
        </h3>
      </blockquote>
    </section>
    <section>
      <h3>Datasets: Ожидание</h3>
      <ul>
        <li>Type-safe operations (как у RDD)</li>
        <li>API унифицирован с DataFrame</li>
        <pre><code class="lang-scala">
  type DataFrame = Dataset[Row]
        </code></pre>
        <li>Tungsten back-end:</li>
        <ul>
          <li>Экономичный off-heap storage</li>
          <li>Компиляция выражений<br>
            <small>(как у DataFrame)</small></li>
          <li>Работа с памятью через Unsafe</li>
          <li>Predicate push-down<br>
            <small>(возможность фильтрации данных на уровне storage)</small>
          </li>
        </ul>
      </ul>
    </section>
    <section>
      <h3>Dataset API</h3>
      <pre><code class="lang-scala">
  // DataFrame operations
  df.filter(col("country") === 12345L)
  df.filter("country = 12345")
  spark.sql("SELECT * FROM df WHERE country = 12345")

  // Dataset operations
  case class UserInfo(
    id: Long,
    country: Long,
    ...
  )
  val ds = df.as[UserInfo]
  ds.filter(_.country == 12345L)
      </code></pre>
    </section>
    <section>
      <h3>Datasets: Exploratory analysis</h3>
        <pre><code class="lang-scala">
  // SQL
  spark.sql("SELECT country, COUNT(*) AS cnt FROM df " +
            "GROUP BY country HAVING cnt >= 100500 " +
            "ORDER BY cnt DESC").show(5)

  // DataFrame
  df.groupBy("country").agg(count("*").as("cnt"))
    .filter(col("cnt") >= 100500)
    .orderBy(-col("cnt"))
    .show(5)

  // Dataset
  ds.groupByKey(_.country).agg(count("*").name("cnt"))
    .filter(_._2 > 100500)
    .orderBy(-col("cnt"))
    .show(5)

        </code></pre>
      <p><small>После агрегации, у Dataset два поля: <code>col("value") & col("cnt")</code>,<br>
        но в Scala-выражениях надо использовать <code>_1 & _2</code>.<br>

      </small></p>
    </section>
    <section>
      <h3>KeyValueGroupedDataset</h3>
        <pre><code class="lang-scala">
  // Map groups
  ds.groupByKey(_.country).mapGroups { (id, iter) =>
    val seq = iter.toSeq.sortBy(_.createTime)
    for (item <- seq) {
      ...
    }
  }

  // Reduce groups
  ds.groupByKey(_.country).reduceGroups { (a, b) =>
    ...
    val t = UserInfo(.....)
    t
  }
        </code></pre>
      <hr>
      <blockquote>
        <h3><small>One does simply write UserDefinedAggregateFunction<br>
          with a single lambda expression</small></h3>
      </blockquote>
    </section>
    <section>
      <div id="size1" style="position: absolute; right: 0; top: 0; width:360px; height:380px;"></div>
      <div style="position: relative; width: 100%;">
        <div style="width: 70%">
          <h3>Tungsten</h3>
          <pre><code class="lang-scala">
  case class UserInfo(
    userId: Int,
    createTime: Long,
    ...)

  val ds = spark.read.parquet(...)
        .as[UserInfo].cache()
  val rdd = ds.rdd.cache()
          </code></pre>
        </div>
      </div>
      <p><small>(Данные <a href="http://snahackathon.org">snahackathon.org</a>)</small></p>
      <hr>
      <blockquote>
        <h4>&mdash; Представление данных Tungsten очень специфично<br>
          &mdash; Расскажи мне о нем</h4>
      </blockquote>
    </section>
    <section>
      <h3>UnsafeRow encoding</h3>
      <div style="max-height: 90%">
        <img src="img/storage.png" style="border: none;box-shadow: none;"/>
      </div>
      <p>&nbsp;</p>
    </section>
    <section>
      <div id="size2" style="position: absolute; right: 0; top: 0; width:360px; height:380px;"></div>
      <div style="position: relative; width: 100%;">
        <div style="width: 70%">
          <h3>Другой пример</h3>
          <pre><code class="lang-scala">
  case class Relation(
    otherId: Int,
    mask: Int)

  case class SimpleUserRelations(
    userId: Int,
    relations: Seq[Relation])
          </code></pre>
        </div>
      </div>
      <p>&nbsp;</p>
      <p><small>(Данные <a href="http://snahackathon.org">snahackathon.org</a>)</small></p>
      <hr>
      <blockquote>
        <h4 style="font-family: 'Comic Sans MS'; position: relative"><small>
          wow<br>concern<br>such encoding<br>very tungsten<br>much memory</small></h4>
      </blockquote>
    </section>
    <section>
      <h3>Advanced implementation</h3>
      <pre><code class="lang-scala">
  case class AdvancedUserRelations(userId: Int,
        otherIds: Array[Int], masks: Array[Int]) {
    require(otherIds.length == masks.length)

    def relations: Iterable[Relation] = new Iterable[Relation] {
      override def iterator = otherIds.indices.iterator.map { i =>
        Relation(otherIds(i), masks(i))
      }
      override def size = otherIds.length
    }
  }

  // Iterate over all data
  ds.map(_.relations.iterator.count(r => (r.mask & (1 << 7)) != 0))
    .reduce(_ + _)
  rdd.map(_.relations.iterator.count(r => (r.mask & (1 << 7)) != 0))
    .reduce(_ + _)
      </code></pre>
    </section>
    <section>
      <h3>Сравнение</h3>
      <div id="size3" style="width:500px; height:380px; display: inline-block;"></div>
      <div id="perf3" style="width:450px; height:380px; display: inline-block;"></div>
    </section>
    <section>
      <h3>Datasets: Реальность</h3>
      <ul>
        <li>API удался, но DF удобнее для exploration</li>
        <li>В некоторых ситуациях, данные в Tungsten encoding занимают в 3 раза меньше памяти:</li>
        <small><ul>
          <li>Sparse (many nulls)</li>
          <li>Strings (UTF8 vs UTF16)</li>
        </ul></small>
        <li>В других ситуациях, JVM-структуры оказываются компактнее:</li>
        <small><ul>
          <li>Primitives</li>
          <li>Primitive arrays</li>
        </ul></small>
        <li>Разница в размере Shuffle не такая критичная, но тенденция сохраняется</li>
        <li>Ручная оптимизация все равно рулит<br>
          <small>PySpark/SparkR компилируют выражения в тот же самый bytecode,
          что и Dataset API.Для скриптовых языков, результат очень хороший,
            но до оптимизированного Scala-кода все еще далеко</small></li>
      </ul>
    </section>
    <section>
      <h3>Datasets: Резюме</h3>
      <p>Для максимальной эффективности<br>
        стоит использовать лучшее из каждой фичи:</p>
      <ul>
        <li>SQL/DF для exploration</li>
        <small><ul>
          <li>исследование схемы данных</li>
          <li>join данных на лету</li>
          <li>статистические ф-ции</li>
        </ul></small>
        <li>Dataset API для type-safe предобработки данных</li>
          <small><ul>
            <li>predicate push-down</li>
            <li>map данных в структуры по имени колонки</li>
            <li>выбор broacast/shuffle join</li>
          </ul></small>
        <li>RDD API для низкоуровневой оптимизации</li>
        <small><ul>
          <li>периодически исполняемые задачи</li>
          <li>ручное управление кешированием</li>
          <li>custom encoding при сохранении/загрузке данных</li>
        </ul></small>
      </ul>
    </section>
    <section>
      <h3>Structured Streaming: Ожидание</h3>
      <pre><code class="lang-scala">
  val users = spark.read.parquet("...")

  val df = spark.read.parquet("...")
  val res = df.joinWith(users, df("userId") === users("id"))
    .filter($"city" === 12345L)
    .groupBy($"age", $"gender").agg(count("*"))

  // Streaming
  val in = spark.readStream.format("json").load("kafka://....")
  val out = df.joinWith(users, df("userId") === users("id"))
    .filter($"city" === 12345L)
    .groupBy($"age", $"gender").agg(count("*"))

  out.writeStream.format("jdbc")
    .trigger(ProcessingTime(10.seconds))
    .queryName("stream1")
    .start("jdbc:mysql....")
      </code></pre>
    </section>
    <section>
      <h3>Structured Streaming: Реальность</h3>
      <ul>
        <li>Удобный API (stateful foreach sink)</li>
        <li>At least once semantics</li>
        <li>Все еще microbatching<br>
        <small>но теперь есть возможность сменить back-end без изменения API (в 2.x)</small></li>
      </ul>
      <hr>
      <div style="margin: 0 auto; display: inline-block;">
        <blockquote class="twitter-tweet" data-lang="en">
          <p lang="en" dir="ltr">Changing nomenclature: Then | now<br>
            low latency | realtime<br>
            online | realtime<br>
            critical path | realtime<br>
            fast | realtime<br>realtime | ...</p>
          &mdash; recursion + negation (@palvaro)
          <a href="https://twitter.com/palvaro/status/741828676069687296">June 12, 2016</a></blockquote>
        <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
      </div>
    </section>
    <section>
      <h3>Improvements</h3>
      <ul>
        <li>ANSI SQL Support<br>
        <small>nested select и прочая экзотика</small></li>
        <li>SparkSession<br>
          <small>вместо SparkContext, SQLContext, HiveContext, StreamingContext</small></li>
        <li>New Accumulator API<br>
          <small>вместо SparkContext, SQLContext, HiveContext, StreamingContext</small></li>
      </ul>
      <pre><code class="lang-scala">
  class Accumulator[T](..., param: AccumulatorParam[T], ...)

  abstract class AccumulatorV2[IN, OUT] {
    def reset(): Unit
    def add(v: IN): Unit
    def merge(other: AccumulatorV2[IN, OUT]): Unit
    def value: OUT
  }
      </code></pre>
    </section>
    <!-- Last Slide -->
    <section>
      <h2>Improvements: stat functions</h2>
      <pre><code class="lang-scala">
  df.stat.approxQuantile("birthDate", Array(0.0, 0.1, 0.5, 0.9, 1.0), 0.01)

  val sketch =  df.stat.countMinSketch("birthDate", 0.0001, 0.99, 12345)
  sketch.estimateCount(6248) // 98
  df.filter("birthDate = 6248").count() // 98

  val bf =  df.stat.bloomFilter("userId", 1000000, 0.01)
  bf.mightContain(51940972)
      </code></pre>
    </section>
    <!-- Last Slide -->
    <section>
      <h2>MLlib</h2>
      <ul>
        <li><code>org.apache.spark.mllib._</code> больше не будет развиваться</li>
        <li>У большинства моделей появился <code>def evaluate()</code><br>
          <small>(характеристики модели на тестовом датасете)</small></li>
        <li>У decision trees появился <code>featureImportances</code><br>
        <small><code>DecisionTreeClassificationModel, RandomForestClassificationModel, GBTClassificationModel, etc
        </code></small></li>
        <li>У <code>MultilayerPerceptronClassifier</code> помимо LBFGS появился SGD</li>
        <li>Добавлен алгоритм кластеризации <code>BisectingKMeans</code></li>
      </ul>
    </section>
    <section>
      <h3>&nbsp;</h3>
      <h3>&nbsp;</h3>
      <h3>Q&A</h3>
      <h3>&nbsp;</h3>
      <h3>&nbsp;</h3>
      <hr>
      <blockquote>
        <h4>&mdash; Релиза пока нет, но вы держитесь!<br>
        Всем хорошего настроения и здоровья!</h4>
      </blockquote>
    </section>
  </div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>
  // More info https://github.com/hakimel/reveal.js#configuration
  Reveal.initialize({
    history: true,
    width: 960,
    height: 600,
    center: false,

    // More info https://github.com/hakimel/reveal.js#dependencies
    dependencies: [
      {src: 'plugin/markdown/marked.js'},
      {src: 'plugin/markdown/markdown.js'},
      {src: 'plugin/notes/notes.js', async: true},
      {
        src: 'plugin/highlight/highlight.js', async: true, callback: function () {
        hljs.initHighlightingOnLoad();
      }
      }
    ]
  });
</script>
<script>
  plotSize1(document.getElementById('size1'));
  plotSize2(document.getElementById('size2'));
  plotSize3(document.getElementById('size3'));

  plotPerf3(document.getElementById('perf3'));
//  var perf_compare3_data = [
//    {
//      x: ['rdd, simple', 'ds, simple', 'rdd, adv', 'ds, adv', 'rdd, array'],
//      y: [0.15, 0.67, 0.57, 0.71, 0.15],
//      type: 'bar'
//    }
//  ];
//  var perf_compare3_layout = {
//    title: "Full iteration (s)",
//    showlegend: false
//  };
//  Plotly.newPlot(perf_compare3, perf_compare3_data, perf_compare3_layout, {displayModeBar: false});
</script>
</body>
</html>
